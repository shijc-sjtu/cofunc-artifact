diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index 80e3fe184..3c87af23d 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -12,7 +12,7 @@ include $(srctree)/virt/kvm/Makefile.kvm
 kvm-y			+= x86.o emulate.o i8259.o irq.o lapic.o \
 			   i8254.o ioapic.o irq_comm.o cpuid.o pmu.o mtrr.o \
 			   hyperv.o debugfs.o mmu/mmu.o mmu/page_track.o \
-			   mmu/spte.o
+			   mmu/spte.o split_container.o
 
 ifdef CONFIG_HYPERV
 kvm-y			+= kvm_onhyperv.o
@@ -27,7 +27,7 @@ kvm-intel-y		+= vmx/vmx.o vmx/vmenter.o vmx/pmu_intel.o vmx/vmcs12.o \
 kvm-intel-$(CONFIG_X86_SGX_KVM)	+= vmx/sgx.o
 
 kvm-amd-y		+= svm/svm.o svm/vmenter.o svm/pmu.o svm/nested.o svm/avic.o \
-			   svm/sev.o svm/hyperv.o
+			   svm/sev.o svm/hyperv.o split_container.o
 
 ifdef CONFIG_HYPERV
 kvm-amd-y		+= svm/svm_onhyperv.o
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index d7847af3e..460af0f45 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -3055,7 +3055,7 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,
 	 * value) and then p*d_offset() walks into the target huge page instead
 	 * of the old page table (sees the new value).
 	 */
-	pgd = READ_ONCE(*pgd_offset(kvm->mm, hva));
+	pgd = READ_ONCE(*pgd_offset(current->mm, hva));
 	if (pgd_none(pgd))
 		goto out;
 
@@ -7131,15 +7131,17 @@ static int kvm_nx_huge_page_recovery_worker(struct kvm *kvm, uintptr_t data)
 
 int kvm_mmu_post_init_vm(struct kvm *kvm)
 {
-	int err;
+	// int err;
 
-	err = kvm_vm_create_worker_thread(kvm, kvm_nx_huge_page_recovery_worker, 0,
-					  "kvm-nx-lpage-recovery",
-					  &kvm->arch.nx_huge_page_recovery_thread);
-	if (!err)
-		kthread_unpark(kvm->arch.nx_huge_page_recovery_thread);
+	// err = kvm_vm_create_worker_thread(kvm, kvm_nx_huge_page_recovery_worker, 0,
+	// 				  "kvm-nx-lpage-recovery",
+	// 				  &kvm->arch.nx_huge_page_recovery_thread);
+	// if (!err)
+	// 	kthread_unpark(kvm->arch.nx_huge_page_recovery_thread);
 
-	return err;
+	// return err;
+	(void)kvm_nx_huge_page_recovery_worker;
+	return 0;
 }
 
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
diff --git a/arch/x86/kvm/split_container.c b/arch/x86/kvm/split_container.c
new file mode 100644
index 000000000..fcd0d5a45
--- /dev/null
+++ b/arch/x86/kvm/split_container.c
@@ -0,0 +1,80 @@
+#include "split_container.h"
+
+#define MAX_N_KVMS 32
+static struct kvm *split_container_kvms[MAX_N_KVMS];
+
+void split_container_vm_create(struct kvm *kvm, unsigned int slot_id)
+{
+	if (slot_id >= MAX_N_KVMS) {
+		return;
+	}
+
+	spin_lock_init(&kvm->sc_idle_vcpus_lock);
+	INIT_LIST_HEAD(&kvm->sc_idle_vcpus);
+	split_container_kvms[slot_id] = kvm;
+}
+
+void split_container_vm_destroy(struct kvm *kvm)
+{
+	int i;
+
+	for (i = 0; i < MAX_N_KVMS; i++) {
+		if (kvm == split_container_kvms[i]) {
+			// printk("[SJC] vm_destroy()\n");
+			split_container_kvms[i] = NULL;
+		}
+	}
+}
+
+struct kvm *split_container_vm_get(unsigned int slot_id)
+{
+	if (slot_id >= MAX_N_KVMS) {
+		return NULL;
+	}
+
+	return split_container_kvms[slot_id];
+}
+
+void split_container_vcpu_idle(struct kvm_vcpu *vcpu)
+{
+	struct kvm *kvm = vcpu->kvm;
+
+	// printk("[SJC] vcpu_idle()\n");
+
+	vcpu->sc_idle = true;
+
+	spin_lock(&kvm->sc_idle_vcpus_lock);
+	list_add(&vcpu->sc_idle_node, &kvm->sc_idle_vcpus);
+	spin_unlock(&kvm->sc_idle_vcpus_lock);
+}
+
+bool split_container_vcpu_is_idle(struct kvm_vcpu *vcpu)
+{
+	return vcpu->sc_idle;
+}
+
+struct kvm_vcpu *split_container_vcpu_alloc(struct kvm *kvm)
+{
+	struct kvm_vcpu *vcpu;
+
+	// printk("[SJC] vcpu_alloc()\n");
+
+	if (!kvm) {
+		return NULL;
+	}
+
+	spin_lock(&kvm->sc_idle_vcpus_lock);
+
+	if (list_empty(&kvm->sc_idle_vcpus)){
+		vcpu = NULL;
+	} else {
+		vcpu = list_first_entry(
+			&kvm->sc_idle_vcpus, struct kvm_vcpu, sc_idle_node);
+		list_del(&vcpu->sc_idle_node);
+		vcpu->sc_idle = false;
+	}
+
+	spin_unlock(&kvm->sc_idle_vcpus_lock);
+
+	return vcpu;
+}
\ No newline at end of file
diff --git a/arch/x86/kvm/split_container.h b/arch/x86/kvm/split_container.h
new file mode 100644
index 000000000..954bba2a2
--- /dev/null
+++ b/arch/x86/kvm/split_container.h
@@ -0,0 +1,22 @@
+#ifndef ARCH_X86_KVM_SPLIT_CONTAINER_H
+#define ARCH_X86_KVM_SPLIT_CONTAINER_H
+
+#include <linux/kvm_host.h>
+
+#define KVM_HC_SC_VCPU_IDLE	100
+#define KVM_HC_SC_REQUEST	101
+
+#define KVM_VM_TYPE_SC_MASK	0xFF000000UL
+#define KVM_VM_TYPE_SC_SLOT_ID(type) (((type >> 24) - 1) & 0xFF)
+
+#define KVM_SC_GET_VM		_IO(KVMIO,    0x10)
+#define KVM_SC_ALLOC_VCPU	_IO(KVMIO,    0x11)
+
+void split_container_vm_create(struct kvm *kvm, unsigned int slot_id);
+void split_container_vm_destroy(struct kvm *kvm);
+struct kvm *split_container_vm_get(unsigned int slot_id);
+void split_container_vcpu_idle(struct kvm_vcpu *vcpu);
+bool split_container_vcpu_is_idle(struct kvm_vcpu *vcpu);
+struct kvm_vcpu *split_container_vcpu_alloc(struct kvm *kvm);
+
+#endif /* ARCH_X86_KVM_SPLIT_CONTAINER_H */
\ No newline at end of file
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 090be7239..9021ce12e 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -4314,6 +4314,7 @@ int sev_handle_vmgexit(struct kvm_vcpu *vcpu)
 	u64 ghcb_gpa, exit_code;
 	struct ghcb *ghcb;
 	int ret;
+	struct mm_struct *mm;
 
 	/* Validate the GHCB */
 	ghcb_gpa = control->ghcb_gpa;
@@ -4327,7 +4328,16 @@ int sev_handle_vmgexit(struct kvm_vcpu *vcpu)
 		return 1;
 	}
 
-	if (kvm_vcpu_map(vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->sev_es.ghcb_map)) {
+	#define SC_GRANTED_MEMORY_BASE 0x180000000
+	if (ghcb_gpa < SC_GRANTED_MEMORY_BASE) {
+		mm = current->mm;
+		current->mm = vcpu->kvm->mm;
+	}
+	ret = kvm_vcpu_map(vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->sev_es.ghcb_map);
+	if (ghcb_gpa < SC_GRANTED_MEMORY_BASE) {
+		current->mm = mm;
+	}
+	if (ret) {
 		/* Unable to map GHCB from guest */
 		vcpu_unimpl(vcpu, "vmgexit: error mapping GHCB [%#llx] from guest\n",
 			    ghcb_gpa);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 027df20a5..c9a9c089e 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1987,10 +1987,16 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	int rc;
+	struct mm_struct *mm;
 
 	u64 fault_address = svm->vmcb->control.exit_info_2;
 	u64 error_code = svm->vmcb->control.exit_info_1;
 
+	#define SC_GRANTED_MEMORY_BASE 0x180000000
+	if (fault_address < SC_GRANTED_MEMORY_BASE) {
+		mm = current->mm;
+		current->mm = vcpu->kvm->mm;
+	}
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
 	rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
 				static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
@@ -2003,10 +2009,14 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 	 */
 	if (error_code & PFERR_GUEST_RMP_MASK) {
 		if (rc == 0)
-			return rc;
+			goto out;
 		handle_rmp_page_fault(vcpu, fault_address, error_code);
 	}
 
+out:
+	if (fault_address < SC_GRANTED_MEMORY_BASE) {
+		current->mm = mm;
+	}
 	return rc;
 }
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 608dcc9bb..0cb90ea31 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -32,6 +32,7 @@
 #include "lapic.h"
 #include "xen.h"
 #include "smm.h"
+#include "split_container.h"
 
 #include <linux/clocksource.h>
 #include <linux/interrupt.h>
@@ -9783,6 +9784,25 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 		vcpu->arch.complete_userspace_io = complete_hypercall_exit;
 		return 0;
 	}
+	case KVM_HC_SC_VCPU_IDLE: {
+		split_container_vcpu_idle(vcpu);
+		vcpu->run->exit_reason        = KVM_EXIT_HYPERCALL;
+		vcpu->run->hypercall.nr       = KVM_HC_SC_VCPU_IDLE;
+		vcpu->run->hypercall.longmode = op_64_bit;
+		vcpu->arch.complete_userspace_io = complete_hypercall_exit;
+		return 0;
+	}
+
+	case KVM_HC_SC_REQUEST: {
+		vcpu->run->exit_reason        = KVM_EXIT_HYPERCALL;
+		vcpu->run->hypercall.nr       = KVM_HC_SC_REQUEST;
+		vcpu->run->hypercall.args[0]  = a0;
+		vcpu->run->hypercall.args[1]  = a1;
+		vcpu->run->hypercall.args[2]  = a2;
+		vcpu->run->hypercall.longmode = op_64_bit;
+		vcpu->arch.complete_userspace_io = complete_hypercall_exit;
+		return 0;
+	}
 	default:
 		ret = -KVM_ENOSYS;
 		break;
@@ -10957,6 +10977,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 	struct kvm_queued_exception *ex = &vcpu->arch.exception;
 	struct kvm_run *kvm_run = vcpu->run;
 	int r;
+	// unsigned long t0, t1;
 
 	vcpu_load(vcpu);
 	kvm_sigset_activate(vcpu);
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 57d56cd09..90b9603a2 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -391,6 +391,9 @@ struct kvm_vcpu {
 	 */
 	struct kvm_memory_slot *last_used_slot;
 	u64 last_used_slot_gen;
+
+	bool sc_idle;
+	struct list_head sc_idle_node;
 };
 
 /*
@@ -838,6 +841,9 @@ struct kvm {
 	struct xarray mem_attr_array;
 #endif
 	char stats_id[KVM_STATS_NAME_SIZE];
+
+	spinlock_t sc_idle_vcpus_lock;
+	struct list_head sc_idle_vcpus;
 };
 
 #define kvm_err(fmt, ...) \
diff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c
index b474289c1..0221f86b0 100644
--- a/kernel/cgroup/cpuset.c
+++ b/kernel/cgroup/cpuset.c
@@ -404,15 +404,16 @@ static struct cpuset top_cpuset = {
  */
 
 DEFINE_STATIC_PERCPU_RWSEM(cpuset_rwsem);
+static DEFINE_MUTEX(cpuset_mutex);
 
 void cpuset_read_lock(void)
 {
-	percpu_down_read(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 }
 
 void cpuset_read_unlock(void)
 {
-	percpu_up_read(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 }
 
 static DEFINE_SPINLOCK(callback_lock);
@@ -1080,7 +1081,7 @@ static void rebuild_root_domains(void)
 	struct cpuset *cs = NULL;
 	struct cgroup_subsys_state *pos_css;
 
-	percpu_rwsem_assert_held(&cpuset_rwsem);
+	lockdep_assert_held(&cpuset_mutex);
 	lockdep_assert_cpus_held();
 	lockdep_assert_held(&sched_domains_mutex);
 
@@ -1141,7 +1142,7 @@ static void rebuild_sched_domains_locked(void)
 	int ndoms;
 
 	lockdep_assert_cpus_held();
-	percpu_rwsem_assert_held(&cpuset_rwsem);
+	lockdep_assert_held(&cpuset_mutex);
 
 	/*
 	 * If we have raced with CPU hotplug, return early to avoid
@@ -1192,9 +1193,9 @@ static void rebuild_sched_domains_locked(void)
 void rebuild_sched_domains(void)
 {
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	rebuild_sched_domains_locked();
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 }
 
@@ -1309,7 +1310,7 @@ static int update_parent_subparts_cpumask(struct cpuset *cs, int cmd,
 	int old_prs, new_prs;
 	int part_error = PERR_NONE;	/* Partition error? */
 
-	percpu_rwsem_assert_held(&cpuset_rwsem);
+	lockdep_assert_held(&cpuset_mutex);
 
 	/*
 	 * The parent must be a partition root.
@@ -1692,7 +1693,7 @@ static void update_sibling_cpumasks(struct cpuset *parent, struct cpuset *cs,
 	struct cpuset *sibling;
 	struct cgroup_subsys_state *pos_css;
 
-	percpu_rwsem_assert_held(&cpuset_rwsem);
+	lockdep_assert_held(&cpuset_mutex);
 
 	/*
 	 * Check all its siblings and call update_cpumasks_hier()
@@ -2448,7 +2449,7 @@ static int cpuset_can_attach(struct cgroup_taskset *tset)
 	cpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset, &css));
 	cs = css_cs(css);
 
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 
 	/* allow moving tasks into an empty cpuset if on default hierarchy */
 	ret = -ENOSPC;
@@ -2478,7 +2479,7 @@ static int cpuset_can_attach(struct cgroup_taskset *tset)
 	cs->attach_in_progress++;
 	ret = 0;
 out_unlock:
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	return ret;
 }
 
@@ -2488,9 +2489,9 @@ static void cpuset_cancel_attach(struct cgroup_taskset *tset)
 
 	cgroup_taskset_first(tset, &css);
 
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	css_cs(css)->attach_in_progress--;
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 }
 
 /*
@@ -2514,7 +2515,7 @@ static void cpuset_attach(struct cgroup_taskset *tset)
 	cs = css_cs(css);
 
 	lockdep_assert_cpus_held();	/* see cgroup_attach_lock() */
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 
 	guarantee_online_mems(cs, &cpuset_attach_nodemask_to);
 
@@ -2566,7 +2567,7 @@ static void cpuset_attach(struct cgroup_taskset *tset)
 	if (!cs->attach_in_progress)
 		wake_up(&cpuset_attach_wq);
 
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 }
 
 /* The various types of files and directories in a cpuset file system */
@@ -2598,7 +2599,7 @@ static int cpuset_write_u64(struct cgroup_subsys_state *css, struct cftype *cft,
 	int retval = 0;
 
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	if (!is_cpuset_online(cs)) {
 		retval = -ENODEV;
 		goto out_unlock;
@@ -2634,7 +2635,7 @@ static int cpuset_write_u64(struct cgroup_subsys_state *css, struct cftype *cft,
 		break;
 	}
 out_unlock:
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 	return retval;
 }
@@ -2647,7 +2648,7 @@ static int cpuset_write_s64(struct cgroup_subsys_state *css, struct cftype *cft,
 	int retval = -ENODEV;
 
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	if (!is_cpuset_online(cs))
 		goto out_unlock;
 
@@ -2660,7 +2661,7 @@ static int cpuset_write_s64(struct cgroup_subsys_state *css, struct cftype *cft,
 		break;
 	}
 out_unlock:
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 	return retval;
 }
@@ -2701,7 +2702,7 @@ static ssize_t cpuset_write_resmask(struct kernfs_open_file *of,
 	flush_work(&cpuset_hotplug_work);
 
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	if (!is_cpuset_online(cs))
 		goto out_unlock;
 
@@ -2725,7 +2726,7 @@ static ssize_t cpuset_write_resmask(struct kernfs_open_file *of,
 
 	free_cpuset(trialcs);
 out_unlock:
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 	kernfs_unbreak_active_protection(of->kn);
 	css_put(&cs->css);
@@ -2873,13 +2874,13 @@ static ssize_t sched_partition_write(struct kernfs_open_file *of, char *buf,
 
 	css_get(&cs->css);
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	if (!is_cpuset_online(cs))
 		goto out_unlock;
 
 	retval = update_prstate(cs, val);
 out_unlock:
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 	css_put(&cs->css);
 	return retval ?: nbytes;
@@ -3092,7 +3093,7 @@ static int cpuset_css_online(struct cgroup_subsys_state *css)
 		return 0;
 
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 
 	set_bit(CS_ONLINE, &cs->flags);
 	if (is_spread_page(parent))
@@ -3143,7 +3144,7 @@ static int cpuset_css_online(struct cgroup_subsys_state *css)
 	cpumask_copy(cs->effective_cpus, parent->cpus_allowed);
 	spin_unlock_irq(&callback_lock);
 out_unlock:
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 	return 0;
 }
@@ -3164,7 +3165,7 @@ static void cpuset_css_offline(struct cgroup_subsys_state *css)
 	struct cpuset *cs = css_cs(css);
 
 	cpus_read_lock();
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 
 	if (is_partition_valid(cs))
 		update_prstate(cs, 0);
@@ -3183,7 +3184,7 @@ static void cpuset_css_offline(struct cgroup_subsys_state *css)
 	cpuset_dec();
 	clear_bit(CS_ONLINE, &cs->flags);
 
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 	cpus_read_unlock();
 }
 
@@ -3196,7 +3197,7 @@ static void cpuset_css_free(struct cgroup_subsys_state *css)
 
 static void cpuset_bind(struct cgroup_subsys_state *root_css)
 {
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 	spin_lock_irq(&callback_lock);
 
 	if (is_in_v2_mode()) {
@@ -3209,7 +3210,7 @@ static void cpuset_bind(struct cgroup_subsys_state *root_css)
 	}
 
 	spin_unlock_irq(&callback_lock);
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 }
 
 /*
@@ -3251,7 +3252,7 @@ struct cgroup_subsys cpuset_cgrp_subsys = {
 
 int __init cpuset_init(void)
 {
-	BUG_ON(percpu_init_rwsem(&cpuset_rwsem));
+	// BUG_ON(percpu_init_rwsem(&cpuset_rwsem));
 
 	BUG_ON(!alloc_cpumask_var(&top_cpuset.cpus_allowed, GFP_KERNEL));
 	BUG_ON(!alloc_cpumask_var(&top_cpuset.effective_cpus, GFP_KERNEL));
@@ -3324,7 +3325,7 @@ hotplug_update_tasks_legacy(struct cpuset *cs,
 	is_empty = cpumask_empty(cs->cpus_allowed) ||
 		   nodes_empty(cs->mems_allowed);
 
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 
 	/*
 	 * Move tasks to the nearest ancestor with execution resources,
@@ -3334,7 +3335,7 @@ hotplug_update_tasks_legacy(struct cpuset *cs,
 	if (is_empty)
 		remove_tasks_in_empty_cpuset(cs);
 
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 }
 
 static void
@@ -3385,14 +3386,14 @@ static void cpuset_hotplug_update_tasks(struct cpuset *cs, struct tmpmasks *tmp)
 retry:
 	wait_event(cpuset_attach_wq, cs->attach_in_progress == 0);
 
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 
 	/*
 	 * We have raced with task attaching. We wait until attaching
 	 * is finished, so we won't attach a task to an empty cpuset.
 	 */
 	if (cs->attach_in_progress) {
-		percpu_up_write(&cpuset_rwsem);
+		mutex_unlock(&cpuset_mutex);
 		goto retry;
 	}
 
@@ -3486,7 +3487,7 @@ static void cpuset_hotplug_update_tasks(struct cpuset *cs, struct tmpmasks *tmp)
 		hotplug_update_tasks_legacy(cs, &new_cpus, &new_mems,
 					    cpus_updated, mems_updated);
 
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 }
 
 /**
@@ -3516,7 +3517,7 @@ static void cpuset_hotplug_workfn(struct work_struct *work)
 	if (on_dfl && !alloc_cpumasks(NULL, &tmp))
 		ptmp = &tmp;
 
-	percpu_down_write(&cpuset_rwsem);
+	mutex_lock(&cpuset_mutex);
 
 	/* fetch the available cpus/mems and find out which changed how */
 	cpumask_copy(&new_cpus, cpu_active_mask);
@@ -3573,7 +3574,7 @@ static void cpuset_hotplug_workfn(struct work_struct *work)
 		update_tasks_nodemask(&top_cpuset);
 	}
 
-	percpu_up_write(&cpuset_rwsem);
+	mutex_unlock(&cpuset_mutex);
 
 	/* if cpus or mems changed, we need to propagate to descendants */
 	if (cpus_updated || mems_updated) {
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 003cb199b..50cec32ad 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -61,6 +61,7 @@
 #include "async_pf.h"
 #include "kvm_mm.h"
 #include "vfio.h"
+#include "split_container.h"
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/kvm.h>
@@ -1352,6 +1353,11 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	if (!kvm)
 		return ERR_PTR(-ENOMEM);
 
+	if (type & KVM_VM_TYPE_SC_MASK) {
+		split_container_vm_create(kvm, KVM_VM_TYPE_SC_SLOT_ID(type));
+		type &= ~KVM_VM_TYPE_SC_MASK;
+	}
+
 	/* KVM is pinned via open("/dev/kvm"), the fd passed to this ioctl(). */
 	__module_get(kvm_chardev_ops.owner);
 
@@ -1503,6 +1509,8 @@ static void kvm_destroy_vm(struct kvm *kvm)
 	int i;
 	struct mm_struct *mm = kvm->mm;
 
+	split_container_vm_destroy(kvm);
+
 	kvm_destroy_pm_notifier(kvm);
 	kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
 	kvm_destroy_vm_debugfs(kvm);
@@ -4425,7 +4433,7 @@ static long kvm_vcpu_ioctl(struct file *filp,
 	struct kvm_fpu *fpu = NULL;
 	struct kvm_sregs *kvm_sregs = NULL;
 
-	if (vcpu->kvm->mm != current->mm || vcpu->kvm->vm_dead)
+	if (vcpu->kvm->vm_dead)
 		return -EIO;
 
 	if (unlikely(_IOC_TYPE(ioctl) != KVMIO))
@@ -4447,6 +4455,8 @@ static long kvm_vcpu_ioctl(struct file *filp,
 		r = -EINVAL;
 		if (arg)
 			goto out;
+		if (split_container_vcpu_is_idle(vcpu))
+			goto out;
 		oldpid = rcu_access_pointer(vcpu->pid);
 		if (unlikely(oldpid != task_pid(current))) {
 			/* The thread running this VCPU changed. */
@@ -4458,8 +4468,8 @@ static long kvm_vcpu_ioctl(struct file *filp,
 
 			newpid = get_task_pid(current, PIDTYPE_PID);
 			rcu_assign_pointer(vcpu->pid, newpid);
-			if (oldpid)
-				synchronize_rcu();
+			// if (oldpid)
+			// 	synchronize_rcu();
 			put_pid(oldpid);
 		}
 		r = kvm_arch_vcpu_ioctl_run(vcpu);
@@ -5082,6 +5092,25 @@ do {										\
 		     sizeof_field(struct kvm_userspace_memory_region2, field));	\
 } while (0)
 
+static int kvm_vm_ioctl_sc_alloc_vcpu(struct kvm *kvm)
+{
+	int fd;
+	struct kvm_vcpu *vcpu;
+
+	vcpu = split_container_vcpu_alloc(kvm);
+	if (vcpu == NULL) {
+		return -ENOENT;
+	}
+
+	kvm_get_kvm(vcpu->kvm);
+	fd = create_vcpu_fd(vcpu);
+	if (fd < 0) {
+		kvm_put_kvm_no_destroy(vcpu->kvm);
+	}
+
+	return fd;
+}
+
 static long kvm_vm_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -5089,7 +5118,7 @@ static long kvm_vm_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	int r;
 
-	if (kvm->mm != current->mm || kvm->vm_dead)
+	if (kvm->vm_dead)
 		return -EIO;
 	switch (ioctl) {
 	case KVM_CREATE_VCPU:
@@ -5304,6 +5333,9 @@ static long kvm_vm_ioctl(struct file *filp,
 	case KVM_GET_STATS_FD:
 		r = kvm_vm_ioctl_get_stats_fd(kvm);
 		break;
+	case KVM_SC_ALLOC_VCPU:
+		r = kvm_vm_ioctl_sc_alloc_vcpu(kvm);
+		break;
 	default:
 		r = kvm_arch_vm_ioctl(filp, ioctl, arg);
 	}
@@ -5447,6 +5479,39 @@ static int kvm_dev_ioctl_create_vm(unsigned long type)
 	return r;
 }
 
+static int kvm_dev_ioctl_sc_get_vm(unsigned long arg)
+{
+	int r;
+	struct kvm *kvm;
+	struct file *file;
+
+	// (void)arg;
+
+	kvm = split_container_vm_get(arg);
+	if (!kvm) {
+		return -ENOENT;
+	}
+
+	r = get_unused_fd_flags(O_CLOEXEC);
+	if (r < 0)
+		goto put_kvm;
+
+	kvm_get_kvm(kvm);
+	file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
+	if (IS_ERR(file)) {
+		put_unused_fd(r);
+		r = PTR_ERR(file);
+		goto put_kvm;
+	}
+
+	fd_install(r, file);
+	return r;
+
+put_kvm:
+	kvm_put_kvm(kvm);
+	return r;
+}
+
 static long kvm_dev_ioctl(struct file *filp,
 			  unsigned int ioctl, unsigned long arg)
 {
@@ -5480,6 +5545,9 @@ static long kvm_dev_ioctl(struct file *filp,
 	case KVM_TRACE_DISABLE:
 		r = -EOPNOTSUPP;
 		break;
+	case KVM_SC_GET_VM:
+		r = kvm_dev_ioctl_sc_get_vm(arg);
+		break;
 	default:
 		return kvm_arch_dev_ioctl(filp, ioctl, arg);
 	}
